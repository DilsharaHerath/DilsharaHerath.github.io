---
title: "FlowSegModel: Weather-Resilient Semantic Segmentation"
status: "Research Publication"
hero: "/images/projects/FlowSeg_MARC/Method.drawio.png"
primary_link_text: "Paper"
primary_link_url: "https://drive.google.com/file/d/19cxAAo216SmKzexCfNDDKUzaRORCupHO/view?usp=drive_link"
# secondary_link_text: "ArXiv"  # Add if preprint available
# secondary_link_url: ""
# group: "Dilshara Herath, Thiwanka Alahakoon, Oshada Rathnayake, Sanjula Senadeera, Roshan Godaliyadda, Parakrama Ekanayake"
Keywords: "Semantic Segmentation, Optical Flow, Adverse Weather, DeepLabV3, RAFT, SEA-RAFT, Autonomous Driving"
excerpt: >
  A novel semantic segmentation methodology integrating RGB images with optical flow inputs for robust performance under adverse weather conditions, achieving 0.8034 mean IoU in fog, rain, and overcast scenarios.
order: 8
---

This research developed a comprehensive methodology for semantic segmentation in adverse weather conditions, employing a FlowSegModel architecture that integrates RGB images with optical flow inputs within a modified DeepLabV3 framework. The system was evaluated on the VKITTI2 dataset, demonstrating robust performance under dynamic weather conditions including fog, rain, and overcast skies.

The FlowSegModel achieves mean Intersection over Union (IoU) scores of 0.8406 under normal conditions and 0.8034 in adverse scenarios, demonstrating only a 4.4% degradation. This resilience is attributed to optical flow's complementary motion cues, which enhance visibility in degraded environments where traditional RGB-only methods struggle.

### Key Components and Methodology

1. **Dataset Preparation**  
   The VKITTI2 synthetic benchmark dataset was utilized, providing RGB image pairs, optical flow ground truth, and segmentation labels across multiple weather conditions (clone/normal, fog, rain, overcast) and camera viewpoints (15°/30° left/right). Flow denormalization was performed using f = 2/(2^16 - 1) × bgr_RG - 1, scaled to pixel coordinates. Images were concatenated into 5-channel tensors (3 RGB + 2 optical flow u/v components) and resized to 384×1248 pixels.

2. **Optical Flow Estimation**  
   Four optical flow estimation models were evaluated: fine-tuned RAFT and SEA-RAFT (deep learning-based), alongside classical Farneback and Horn-Schunck methods. Deep models were pretrained on standard benchmarks and fine-tuned on VKITTI2. Evaluation metrics included End-Point Error (EPE = sqrt((u - u_gt)^2 + (v - v_gt)^2)), F1-all outlier rate (EPE > 3 pixels and relative error > 0.05), and precision thresholds (px<1, px<3, px<5). SEA-RAFT achieved superior performance with EPE of 0.7758 after fine-tuning, compared to RAFT's 1.7861 and classical methods exceeding 512.

3. **Segmentation Architecture (FlowSegModel)**  
   A modified DeepLabV3+ architecture with ResNet-101 backbone was implemented. The initial convolutional layer was reconfigured from 3 to 5 input channels to accommodate the RGB-flow fusion. The Atrous Spatial Pyramid Pooling (ASPP) module was optimized with adjusted dilation rates to capture multi-scale context under degraded visibility. The classifier head was adapted for 7-class pixel-wise predictions. Training utilized AdamW optimizer with learning rate 1e-4, cross-entropy loss, batch size 16, and early stopping based on validation mIoU over 20 epochs.

4. **Cyclostationary Analysis**  
   Spectral Correlation Density (SCD) was computed from denoised signals via Spectral Correlation Function: S_x^α(f) = lim(T→∞) (1/T) × <X_T(f + α/2) · X_T*(f - α/2)>. This analysis generated 2D SCD images that exploit the periodic nature of motor emissions, providing robust features for classification.

5. **Evaluation and Ablation Studies**  
   Performance was assessed using mean IoU (mIoU = (1/C) × Σ TP_c/(TP_c + FP_c + FN_c)), along with macro-averaged precision, recall, and F1-score. Comparative analyses revealed that fine-tuned deep learning models outperformed classical methods by over 96% in EPE reduction. An ablation study using an optical-flow-only variant demonstrated a 46.1% mean IoU improvement when fusing RGB and flow, confirming that flow-alone struggles with static scene discrimination absent color and texture data.

## Technical Contributions

- Pioneered a 5-channel fusion architecture adapting DeepLabV3+ for RGB-optical flow integration, achieving weather-resilient segmentation.
- Demonstrated SEA-RAFT's superiority over RAFT and classical methods, with 56.6% better EPE performance through weather-aware design and Mixture-of-Laplace loss optimization.
- Conducted comprehensive ablation studies quantifying RGB-flow synergy, showing flow-only variants achieve only 0.1431 IoU for vehicles versus 0.8375 for the full model.
- Validated end-to-end pipeline on VKITTI2 with PyTorch implementation on NVIDIA A100 GPUs, achieving minimal 4.4% mIoU degradation under adverse conditions.
- Advanced autonomous driving perception with practical implications for weather-invariant navigation and reduced real-world failure rates.

## Visual Results

<div class="project-gallery" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
  <figure>
    <img src="/images/projects/FlowSeg_MARC/Method.drawio.png" alt="FlowSegModel system architecture" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>Complete FlowSegModel architecture showing 5-channel fusion into DeepLabV3+ encoder-decoder framework</figcaption>
  </figure>
  <figure>
    <img src="/images/projects/FlowSeg_MARC/deeplab.png" alt="DeepLabV3+ encoder-decoder structure" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>DeepLabV3+ encoder-decoder architecture with ResNet-101 backbone and ASPP module</figcaption>
  </figure>
  <figure>
    <img src="/images/projects/FlowSeg_MARC/flow1.png" alt="Optical flow visualizations across weather conditions" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>Optical flow comparison: a) RAFT vs b)SEA-RAFT under Clone, Fog, Overcast, and Rain conditions</figcaption>
  </figure>
  <figure>
    <img src="/images/projects/FlowSeg_MARC/Segmentation_Results.jpg" alt="Segmentation predictions vs ground truth" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>Segmentation results across varying weather: RGB inputs, ground truth masks, and FlowSegModel predictions</figcaption>
  </figure>
</div>

## Performance Metrics

**Optical Flow Comparison Across Methods**

| Model | Condition | EPE | F1-all | px<1 | px<3 | px<5 |
|-------|-----------|-----|--------|------|------|------|
| **SEA-RAFT** | Pre-trained | 13.1929 | 60.18% | 30.02% | 65.42% | 76.81% |
| **SEA-RAFT** | Fine-tuned | **0.7758** | **3.67%** | **89.73%** | **95.48%** | **98.78%** |
| **RAFT** | Pre-trained | 19.6173 | 76.59% | 13.14% | 23.41% | 31.83% |
| **RAFT** | Fine-tuned | 1.7861 | 10.67% | 67.58% | 88.92% | 94.02% |
| Farneback | N/A | 512.1124 | 100% | 0% | 0% | 0% |
| Horn-Schunck | N/A | 512.4921 | 100% | 0% | 0% | 0% |

**Segmentation Performance by Class**

| Class | Normal IoU | Normal F1 | Adverse IoU | Adverse F1 |
|-------|-----------|-----------|-------------|-----------|
| Background | 0.9099 | 0.9528 | 0.8731 | 0.9323 |
| Vehicle | 0.8011 | 0.8896 | **0.8375** | 0.9116 |
| Road | 0.8109 | 0.8956 | 0.6996 | 0.8232 |
| **Mean IoU** | **0.8406** | - | **0.8034** | - |

**Ablation Study: Flow-Only vs Full RGB-Flow Model**

| Class | Flow-Only IoU | Full Model IoU | Improvement |
|-------|---------------|----------------|-------------|
| Background | 0.7115 | 0.8731 | +22.7% |
| Vehicle | 0.1431 | 0.8375 | +485% |
| Road | 0.4753 | 0.6996 | +47.2% |

## Acknowledgments

This work was conducted at the Multidisciplinary AI Research Centre (MARC), University of Peradeniya, Sri Lanka, under the guidance of Prof. Roshan Godaliyadda and Prof. Parakrama Ekanayake from the Department of Electrical and Electronic Engineering. Collaborative contributions were made by Thiwanka Alahakoon, Oshada Rathnayake, and Sanjula Senadeera.





<!-- ---
title: "FlowSegModel: Weather-Resilient Semantic Segmentation"
status: "Research Publication"
hero: "/images/projects/FlowSeg_MARC/Method.drawio.png"
primary_link_text: "Paper"
primary_link_url: "https://drive.google.com/file/d/19cxAAo216SmKzexCfNDDKUzaRORCupHO/view?usp=drive_link"
# secondary_link_text: "ArXiv"  # Add if preprint available
# secondary_link_url: ""
Keywords: "Semantic Segmentation, Optical Flow, Adverse Weather, DeepLabV3, RAFT, SEA-RAFT, Autonomous Driving"
excerpt: >
  Fuses RGB and SEA-RAFT optical flow into 5-channel DeepLabV3+ for robust segmentation on VKITTI2, achieving 0.8034 mIoU in fog/rain (4.4% degradation).
order: 8
---

## Project Overview

Developed FlowSegModel, a modified DeepLabV3+ (ResNet-101) fusing 3-channel RGB with 2-channel optical flow (from fine-tuned SEA-RAFT) for semantic segmentation under adverse weather (fog, rain, overcast). On VKITTI2 dataset, attains mIoU of 0.8406 (normal) and 0.8034 (adverse)—minimal 4.4% drop—via motion cues compensating visibility loss. SEA-RAFT optical flow outperforms RAFT/classical methods (Farneback/Horn-Schunck) by 96%+ EPE reduction; ablation shows RGB+flow 46.1% mIoU gain over flow-only.

### Key Components and Methodology

1. **Dataset Preparation**  
   VKITTI2 synthetic benchmark: RGB pairs, flow GT, segmentation labels across weather (clone/normal, fog, rain, overcast, etc.) and views (15°/30° left/right). Flow denormalized: \( f = \frac{2}{2^{16}-1} \cdot \text{bgr}_{RG} - 1 \), scaled to pixels; 5-channel concat (RGB+flow u/v); resize 384×1248.

2. **Optical Flow Estimation**  
   Fine-tuned RAFT/SEA-RAFT (pretrained + VKITTI2); classical Farneback/Horn-Schunck direct eval. Metrics: EPE \( \sqrt{(u - u_{gt})^2 + (v - v_{gt})^2} \); F1-all outliers (EPE>3 & relative>0.05).

3. **Segmentation Architecture (FlowSegModel)**  
   DeepLabV3+ encoder-decoder: First conv 3→5 channels; ASPP with tuned dilations; DeepLabHead (2048→7 classes). Train: AdamW (1e-4 LR), CE loss, 20 epochs early-stop (mIoU val), batch=16.

4. **Evaluation Metrics**  
   mIoU \( \frac{1}{C} \sum_c \frac{TP_c}{TP_c + FP_c + FN_c} \); macro Precision/Recall/F1. SEA-RAFT MoL loss: \( L_{MoL} = -\frac{1}{2HW} \sum \log[\text{MixLap}] \).

## Technical Contributions

- Customized DeepLabV3+ for 5-channel RGB+flow input, resilient to weather via motion fusion.  
- Fine-tuned SEA-RAFT/RAFT: EPE 0.78/1.79 (fine-tuned) vs. 512+ classical; SEA-RAFT 56.6% better than RAFT.  
- Weather-robust mIoU (0.80 adverse); ablation validates fusion (flow-only IoU drops 19-82%).  
- Advances AV perception; PyTorch impl on A100 GPUs.

## Visual Results

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 2rem 0;">
  <figure>
    <img src="/images/projects/FlowSeg_MARC/Method.drawio.png" alt="FlowSegModel architecture overview" style="width: 100%; height: auto; min-height: 400px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); object-fit: cover;">
    <figcaption>Architectural overview of the FlowSegModel</figcaption>
  </figure>
  <figure>
    <img src="/images/projects/FlowSeg_MARC/deeplab.png" alt="DeepLabV3+ encoder-decoder" style="width: 100%; height: auto; min-height: 400px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); object-fit: cover;">
    <figcaption>Encoder-Decoder architecture of DeepLabV3+, the semantic segmentation framework used in our FlowSegModel</figcaption>
  </figure>
</div>

<div class="project-gallery" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
  <figure>
    <img src="/images/projects/FlowSeg_MARC/flow1.png" alt="Optical flow under weather variants" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>Optical flow visualization for a single scene under varying weather conditions</figcaption>
  </figure>
  <figure>
    <img src="/images/projects/FlowSeg_MARC/Segmentation_Results.jpg" alt="Segmentation predictions" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <figcaption>RGB/GT vs. predicted masks across weather</figcaption>
  </figure>
</div>

### Optical Flow Comparison (Full Width Table)
| Model       | Condition  | EPE     | F1-all | px<1   | px<3   | px<5    |
|-------------|------------|---------|--------|--------|--------|--------|
| **SEA-RAFT** | Pre-trained | 13.1929 | 60.18% | 30.02% | 65.42% | 76.81% |
|             | Fine-tuned  | **0.7758** | **3.67%** | **89.73%** | **95.48%** | **98.78%** |
| **RAFT**     | Pre-trained | 19.6173 | 76.59% | 13.14% | 23.41% | 31.83% |
|             | Fine-tuned  | 1.7861  | 10.67% | 67.58% | 88.92% | 94.02% |
| Farneback  | N/A        | 512.1124| 100%   | 0%     | 0%     | 0%     |
| Horn-Schunck| N/A       | 512.4921| 100%   | 0%     | 0%     | 0%     |

### Segmentation Metrics by Class (Full Width Table)
| Class      | Normal IoU | Normal F1 | Adverse IoU | Adverse F1  |
|------------|------------|-----------|--------------|----------------------|
| Background| 0.9099    | 0.9528   | 0.8731      | 0.9323              |
| Vehicle   | 0.8011    | 0.8896   | **0.8375**  | 0.9116              |
| Road      | 0.8109    | 0.8956   | 0.6996      | 0.8232              |
| **mIoU**  | **0.8406**| -        | **0.8034**  | -                   |

### Ablation: Flow-Only vs. Full Model (Adverse)
| Class      | Flow-Only IoU | Full Model IoU | Gain  |
|------------|---------------|----------------|---------------|
| Background| 0.7115       | 0.8731        | +19.7%       |
| Vehicle   | 0.1431       | 0.8375        | +82.3%       |
| Road      | 0.4753       | 0.6996        | +36.3%       |

## Acknowledgments

Conducted at Multidisciplinary AI Research Centre (MARC), University of Peradeniya, under Prof. Roshan Godaliyadda and Prof. Parakrama Ekanayake. -->
